{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "X1gpl4LtgfNJ"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filters\n",
    "import os\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col, year, month, weekofyear, avg, to_date\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghVPxXGMqGwP"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9Ejtyo-ubDb"
   },
   "source": [
    "\n",
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vVVzyFlnYAJ"
   },
   "source": [
    "**Function :** read_file(folder,file)<br>\n",
    "\n",
    "**Parameters :**<br>\n",
    "folder : path of the folder with the csv files<br>\n",
    "file : name of the file<br>\n",
    "\n",
    "**Return :** spark dataframe<br>\n",
    "\n",
    "**Usage :**<br>\n",
    "This function allow us to load the file into a spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "FPXjJG8LPHAz"
   },
   "outputs": [],
   "source": [
    "def read_file(folder,file):\n",
    "  name = os.path.join(folder, file)\n",
    "  df = spark.read.csv(name,inferSchema=True, header =True)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmJUAbY6nYAO"
   },
   "source": [
    "**Function :** calculate_time_span(dataset)<br>\n",
    "\n",
    "**Parameters :**<br>\n",
    "dataset : spark dataframe<br>\n",
    "\n",
    "**Return :** earliest day, latest day, number of day in the span<br>\n",
    "\n",
    "**Usage :**<br>\n",
    "This function gives the minimum and maximum day, as well as the number of days in a dataframe column with datetimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "NF2mGgCB-hX-"
   },
   "outputs": [],
   "source": [
    "def calculate_time_span(dataset):\n",
    "  dataset2 = dataset.withColumn(\"Date\", to_date(col(\"Date\"), \"yyyy-MM-dd\"))\n",
    "  min = dataset2.agg({\"Date\": \"min\"}).collect()[0][0]\n",
    "  max = dataset2.agg({\"Date\": \"max\"}).collect()[0][0]\n",
    "\n",
    "  day_difference = max - min\n",
    "\n",
    "  return min, max, day_difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXGYDpKpnYAQ"
   },
   "source": [
    "**Function :** calcultate_corr(dataset)<br>\n",
    "\n",
    "**Parameters :**<br>\n",
    "dataset : spark dataframe<br>\n",
    "\n",
    "**Return :** a correlation matrix<br>\n",
    "\n",
    "**Usage :**<br>\n",
    "This function gives the correlation matrix for a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ZzyahzwN9vko"
   },
   "outputs": [],
   "source": [
    "def calcultate_corr(dataset):\n",
    "  # convert to vector\n",
    "  vector_col = \"correlations\"\n",
    "  numeric = dataset.drop(*[\"Date\", \"company_name\"])\n",
    "\n",
    "  assembler = VectorAssembler(inputCols=numeric.columns, outputCol=vector_col)\n",
    "  output = assembler.transform(numeric)\n",
    "\n",
    "  matrix = Correlation.corr(output, vector_col)\n",
    "  matrix2 = matrix.collect()[0]['pearson({})'.format(vector_col)].values\n",
    "\n",
    "  num_columns = len(numeric.columns)\n",
    "  correlation_matrix = np.array(matrix2).reshape((num_columns, num_columns))\n",
    "\n",
    "  return correlation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRgOHiEznYAR"
   },
   "source": [
    "**Function :** get_info_df(dataset)<br>\n",
    "\n",
    "**Parameters :**<br>\n",
    "dataset : spark dataframe<br>\n",
    "file : name of the file<br>\n",
    "\n",
    "**Return :** null<br>\n",
    "\n",
    "**Usage :**<br>\n",
    "Gives many insights about a dataframe: schema, top 10 rows, number of values, time span the data is about, data type for each column, missing values, correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "F6CVqxSmogTE"
   },
   "outputs": [],
   "source": [
    "def get_info_df(dataset,file):\n",
    "  print(f\"Infos of the {file} file \\n\\n\")\n",
    "\n",
    "  print(\"► Schema of the dataset\")\n",
    "  print(dataset.printSchema())\n",
    "\n",
    "  print(\"\\n ► Top rows of the dataset\")\n",
    "  print(dataset.show(10))\n",
    "\n",
    "  print(\"\\n ► Row count\")\n",
    "  print(dataset.count())\n",
    "\n",
    "  print(\"\\n ► Time span\")\n",
    "  min_date, max_date, span = calculate_time_span(dataset)\n",
    "  print(f\"Minimum date {min_date}\")\n",
    "  print((f\"Maximum date {max_date}\"))\n",
    "  print(span)\n",
    "\n",
    "  print(\"\\n ► Columns info\")\n",
    "  for name, col in zip(dataset.schema.names, dataset.columns):\n",
    "    print(name)\n",
    "    print(dataset.describe([col]).show())\n",
    "\n",
    "  print(\"\\n ► Missing values for each column\")\n",
    "  for col in dataset.columns:\n",
    "    print(col, \"\\t\", \"number of null values: \", dataset.filter(dataset[col].isNull()).count())\n",
    "\n",
    "  print(\"\\n ► Correlation Matrix\")\n",
    "  correlation = calcultate_corr(dataset)\n",
    "  print(correlation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EpBSlPYnYAT"
   },
   "source": [
    "**Function :** get_average_year(dataset, column)<br>\n",
    "\n",
    "**Parameters :**<br>\n",
    "dataset : spark dataframe<br>\n",
    "column : a column of the dataframe<br>\n",
    "\n",
    "**Return :** a dataframe column grouped by year <br>\n",
    "\n",
    "**Usage :**<br>\n",
    "Gives the average value of a column per year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "RJRI65W8r8kG"
   },
   "outputs": [],
   "source": [
    "def get_average_year(dataset, column):\n",
    "  poulet = dataset.withColumn(\"Date\", col(\"Date\").cast(\"Date\"))\n",
    "\n",
    "  toAverage = poulet.groupBy(year(\"date\").alias(\"year\")).agg(avg(column).alias(f\"average_{column}\"))\n",
    "  toAverage = toAverage.orderBy(year(\"date\"))\n",
    "\n",
    "  return toAverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EgQ_x9llnYAX"
   },
   "source": [
    "**Function :** get_average_month(dataset, column)<br>\n",
    "\n",
    "**Parameters :**<br>\n",
    "dataset : spark dataframe<br>\n",
    "column : a column of the dataframe<br>\n",
    "\n",
    "**Return :** a dataframe column grouped by month <br>\n",
    "\n",
    "**Usage :**<br>\n",
    "Gives the average value of a column per month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Qro44a_lssFQ"
   },
   "outputs": [],
   "source": [
    "def get_average_month(dataset, column):\n",
    "  poulet = dataset.withColumn(\"Date\", col(\"Date\").cast(\"Date\"))\n",
    "\n",
    "  toAverage = poulet.groupBy(year(\"date\").alias(\"year\"), month(\"date\").alias(\"month\")).agg(avg(column).alias(f\"average_{column}\"))\n",
    "  toAverage = toAverage.orderBy(year(\"date\"), month(\"date\"))\n",
    "\n",
    "  return toAverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nyDxaUCnYAY"
   },
   "source": [
    "**Function :** get_average_week(dataset, column)<br>\n",
    "\n",
    "**Parameters :**<br>\n",
    "dataset : spark dataframe<br>\n",
    "column : a column of the dataframe<br>\n",
    "\n",
    "**Return :** a dataframe column grouped by week <br>\n",
    "\n",
    "**Usage :**<br>\n",
    "Gives the average value of a column per week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "I-THyN7fmyDQ"
   },
   "outputs": [],
   "source": [
    "def get_average_week(dataset, column):\n",
    "  poulet = dataset.withColumn(\"Date\", col(\"Date\").cast(\"Date\"))\n",
    "\n",
    "  toAverage = poulet.groupBy(year(\"date\").alias(\"year\"), weekofyear(\"date\").alias(\"week\")).agg(avg(column).alias(f\"average_{column}\"))\n",
    "  toAverage = toAverage.orderBy(year(\"date\"), weekofyear(\"date\"))\n",
    "\n",
    "  return toAverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vM7KeLZnYAd"
   },
   "source": [
    "**Function :** daily_difference(dataset)<br>\n",
    "\n",
    "**Parameters :**<br>\n",
    "dataset : spark dataframe<br>\n",
    "\n",
    "**Return :** spark dataframe<br>\n",
    "\n",
    "**Usage :**<br>\n",
    "Returns a new dataframe with the column \"stock_difference\" being the difference on closing values over 2 consecutive days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "1sAIeH4XmDeR"
   },
   "outputs": [],
   "source": [
    "def daily_difference(dataset):\n",
    "  windowSpec = Window().orderBy(\"date\")\n",
    "\n",
    "  dataset_day_diff = dataset.withColumn(\"stock_difference\", F.col(\"Close\") - F.lag(\"Close\").over(windowSpec))\n",
    "\n",
    "  return dataset_day_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iySrrD0VnYAg"
   },
   "source": [
    "**Function :** monthly_difference(dataset)<br>\n",
    "\n",
    "**Parameters :**<br>\n",
    "dataset : spark dataframe<br>\n",
    "\n",
    "**Return :** spark dataframe<br>\n",
    "\n",
    "**Usage :**<br>\n",
    "Returns a new dataframe with the column \"stock_difference\" being the difference on closing values over 2 consecutive months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "VPdagZji8YDr"
   },
   "outputs": [],
   "source": [
    "def monthly_difference(dataset):\n",
    "  window_spec = Window.partitionBy(F.year(\"date\"), F.month(\"date\")).orderBy(\"date\")\n",
    "\n",
    "  df_with_row_number = dataset.withColumn(\"row_number\", F.row_number().over(window_spec))\n",
    "  result_df = df_with_row_number.filter(F.col(\"row_number\") == 1).drop(\"row_number\")\n",
    "\n",
    "  windowSpec2 = Window().orderBy(\"date\")\n",
    "\n",
    "  dataset_month_diff = result_df.withColumn(\"stock_difference\", F.col(\"Close\") - F.lag(\"Close\").over(windowSpec2))\n",
    "\n",
    "  return dataset_month_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yEsa5JulnYAh"
   },
   "source": [
    "**Function :** calculate_daily_return(dataset)<br>\n",
    "\n",
    "**Parameters :**<br>\n",
    "dataset : spark dataframe<br>\n",
    "\n",
    "**Return :** spark dataframe<br>\n",
    "\n",
    "**Usage :**<br>\n",
    "Returns a new dataframe with the column \"Daily_return\" being the daily return of a stock (difference of closing and opening values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ywWp1o0pytM6"
   },
   "outputs": [],
   "source": [
    "def calculate_daily_return(dataset):\n",
    "  newDataset = dataset.withColumn(\"Daily_return\",  col(\"Close\") - col(\"Open\"))\n",
    "\n",
    "  return newDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50Ff7XTInYAm"
   },
   "source": [
    "**Function :** get_highest_dr(dataset, start_date=None, end_date=None)<br>\n",
    "\n",
    "**Parameters :**<br>\n",
    "dataset : spark dataframe<br>\n",
    "start_date (optional): give the start date for a time span to look after<br>\n",
    "end_date (optional): give the end date for a time span to look after<br>\n",
    "\n",
    "**Return :** spark dataframe<br>\n",
    "\n",
    "**Usage :**<br>\n",
    "Function giving the highest daily returns over a time period. If no start and end date are inputted as parameters, the function will simply take the whole dataset by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Zqi_XRaFka0k"
   },
   "outputs": [],
   "source": [
    "def get_highest_dr(dataset, start_date=None, end_date=None):\n",
    "    if \"Daily_return\" in dataset.columns:\n",
    "        if start_date is not None and end_date is not None:\n",
    "            filteredDataset = dataset.filter((col(\"Date\") >= start_date) & (col(\"Date\") <= end_date))\n",
    "            sortedAndFilteredDataset = filteredDataset.orderBy(col(\"Daily_return\").desc())\n",
    "            return sortedAndFilteredDataset\n",
    "        else:\n",
    "            sortedDataset = dataset.orderBy(col(\"Daily_return\").desc())\n",
    "            return sortedDataset\n",
    "    else:\n",
    "        print(\"Error: 'Daily_return' column not found.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oAgHbD9knYAu"
   },
   "source": [
    "**Function :** calculate_moving_average(df, column_name, n, start_date=None)<br>\n",
    "\n",
    "**Parameters :**<br>\n",
    "df : spark dataframe<br>\n",
    "column_name : name of the column to use the rolling average technique on<br>\n",
    "n : number of points to use for the moving average<br>\n",
    "start_date (optional): give the reference date as a starting point for the moving average<br>\n",
    "\n",
    "**Return :** spark dataframe<br>\n",
    "\n",
    "**Usage :**<br>\n",
    "This function calculates the moving average starting a defined date and with a certain number of n points. It then displays the results in a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "4pnMkQGMA5DX"
   },
   "outputs": [],
   "source": [
    "def calculate_moving_average(df, column_name, n, start_date=None):\n",
    "\n",
    "    window_spec = Window.orderBy(\"date\").rowsBetween(-n + 1, 0)\n",
    "\n",
    "    moving_avg_col = f\"{column_name}_moving_avg\"\n",
    "    df_with_moving_avg = df.withColumn(moving_avg_col, F.avg(column_name).over(window_spec))\n",
    "\n",
    "    if start_date is not None:\n",
    "        result_df = df_with_moving_avg.filter(F.col(\"date\") >= start_date)\n",
    "    else:\n",
    "        result_df = df_with_moving_avg\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8OofZGKnYAy"
   },
   "source": [
    "**Function :** get_ror_daytoday(dataset, reference_date, period)<br>\n",
    "\n",
    "**Parameters :**<br>\n",
    "dataset : spark dataframe<br>\n",
    "reference_date : reference date to calculate the rate of return<br>\n",
    "period : integer, number of day to calculate the RoR on<br>\n",
    "\n",
    "**Return :** spark dataframe<br>\n",
    "\n",
    "**Usage :**<br>\n",
    "This function calculates the return rate on a whole period based on the value of the reference date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "1bZj5qaNDknf"
   },
   "outputs": [],
   "source": [
    "def get_ror_period(dataset, reference_date, period):\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    dataset = dataset.withColumn(\"date\", F.to_date(\"date\"))\n",
    "\n",
    "    end_date = reference_date + F.expr(f\"INTERVAL {period} DAYS\")\n",
    "\n",
    "    filtered_data = dataset.filter((F.col(\"date\") >= reference_date) & (F.col(\"date\") <= end_date))\n",
    "\n",
    "    return_rate_column = \"return_rate\"\n",
    "    initial_close_price = filtered_data.filter(F.col(\"date\") == reference_date).select(\"close\").collect()[0][0]\n",
    "\n",
    "    dataset = (filtered_data\n",
    "               .withColumn(return_rate_column,\n",
    "                           (F.col(\"close\") - F.lit(initial_close_price)) / F.lit(initial_close_price))\n",
    "               .orderBy(\"date\"))\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84WjtUibnYAz"
   },
   "source": [
    "**Function :** calculate_stock_correlation(stock1, stock2, start_date=None, end_date=None)<br>\n",
    "\n",
    "**Parameters :**<br>\n",
    "stock1 : spark dataframe<br>\n",
    "stock2 : spark dataframe<br>\n",
    "start_date (optional): give the start date for a time span to look after<br>\n",
    "end_date (optional): give the end date for a time span to look after<br>\n",
    "\n",
    "**Return :** correlation coefficient<br>\n",
    "\n",
    "**Usage :**<br>\n",
    "This function returns the correlation coefficient for 2 stocks on a given time span."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "BiNGzXwW5ft_"
   },
   "outputs": [],
   "source": [
    "def calculate_stock_correlation(stock1, stock2, start_date=None, end_date=None):\n",
    "\n",
    "    stock1 = stock1.select(\"date\", \"close\").withColumnRenamed(\"close\", \"close1\")\n",
    "    stock2 = stock2.select(\"date\", \"close\").withColumnRenamed(\"close\", \"close2\")\n",
    "\n",
    "    if start_date and end_date:\n",
    "        stock1 = stock1.filter((F.col(\"date\") >= start_date) & (F.col(\"date\") <= end_date))\n",
    "        stock2 = stock2.filter((F.col(\"date\") >= start_date) & (F.col(\"date\") <= end_date))\n",
    "\n",
    "    joined_data = stock1.join(stock2, \"date\", \"inner\")\n",
    "\n",
    "    correlation = joined_data.corr(\"close1\", \"close2\")\n",
    "\n",
    "    return correlation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9X0reOhfnYBA"
   },
   "source": [
    "**Function :** get_trading_value(dataset)<br>\n",
    "\n",
    "**Parameters :**<br>\n",
    "dataset : spark dataframe<br>\n",
    "\n",
    "**Return :** spark dataframe<br>\n",
    "\n",
    "**Usage :**<br>\n",
    "Returns a new dataframe with the column \"trading_value\" being the actual total trading value for the day (closing value for one stock * volume of stocks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "psB1fhjUO_8u"
   },
   "outputs": [],
   "source": [
    "def get_trading_value(dataset):\n",
    "\n",
    "  df = dataset.withColumn(\"trading_value\", col(\"Close\") * col(\"Volume\"))\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDIPPJIznYBC"
   },
   "source": [
    "**Function :** plot_stock()<br>\n",
    "\n",
    "**Parameters :**<br>\n",
    "None <br>\n",
    "\n",
    "**Return :** plotly plot<br>\n",
    "\n",
    "**Usage :**<br>\n",
    "Generate a plotly graph object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "YbUYVhhYOYMA"
   },
   "outputs": [],
   "source": [
    "def plot_stock():\n",
    "  fig = go.Figure()\n",
    "  fig.update_layout(title='Stocks over time span', xaxis_title='Date', yaxis_title='Close')\n",
    "\n",
    "  return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tN58XSdonYBD"
   },
   "source": [
    "**Function :** add_trace_plot(dataset, fig)<br>\n",
    "\n",
    "**Parameters :**<br>\n",
    "dataset : spark dataframe <br>\n",
    "column : column from the dataframe <br>\n",
    "fig : plotly graph object <br>\n",
    "name : name to give to the trace <br>\n",
    "\n",
    "**Return :** updated plotly GO<br>\n",
    "\n",
    "**Usage :**<br>\n",
    "Adds a line trace to a plotly graph object. Useful to plot the stock evolution for each company and on the same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "QPfyRFxOSEzW"
   },
   "outputs": [],
   "source": [
    "def add_trace_plot(dataset, column, fig, name):\n",
    "  df_pandas = dataset.toPandas()\n",
    "  fig.add_trace(go.Scatter(x=df_pandas['Date'], y=df_pandas[column], mode='lines', name=name))\n",
    "  return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTdltAtxzOts"
   },
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
